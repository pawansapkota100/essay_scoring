{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/1649372495.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# from gensim.models import Word2Vec\n",
    "from collections.abc import Mapping\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawansapkota/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout,Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# def get_model():\n",
    "#     \"\"\"Define the model.\"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))\n",
    "#     model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "#     model.summary()\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 10ms/step - loss: 64.8860 - mae: 4.4087\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 41.0067 - mae: 3.6360\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 34.3757 - mae: 3.4892\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 31.0910 - mae: 3.4005\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 29.3748 - mae: 3.2806\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 27.4783 - mae: 3.0898\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 25.3883 - mae: 2.9013\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 21.7307 - mae: 2.6643\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 19.1727 - mae: 2.4857\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 17.7441 - mae: 2.3928\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 15.6685 - mae: 2.2582\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 15.8442 - mae: 2.2293\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 14.8354 - mae: 2.1855\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 13.7283 - mae: 2.0906\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 13.5611 - mae: 2.0796\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 13.0697 - mae: 2.0375\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 13.1770 - mae: 2.0245\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 12.1223 - mae: 1.9468\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 11.4680 - mae: 1.9005\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 11.9351 - mae: 1.9209\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.6019 - mae: 1.8899\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.6673 - mae: 1.8743\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.0242 - mae: 1.8453\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.5157 - mae: 1.7889\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.9911 - mae: 1.8118\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.5212 - mae: 1.7954\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 10.2297 - mae: 1.7660\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.0706 - mae: 1.7729\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.1427 - mae: 1.7683\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.6496 - mae: 1.7796\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.0468 - mae: 1.7451\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 9.8950 - mae: 1.7400\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 9.5000 - mae: 1.7258\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 9.5421 - mae: 1.7236\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.4956 - mae: 1.7110\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 9.2395 - mae: 1.6991\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.5930 - mae: 1.7061\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 9.3997 - mae: 1.6976\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.1542 - mae: 1.7149\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2556 - mae: 1.6976\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 9.3242 - mae: 1.6880\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 9.0821 - mae: 1.6855\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.8056 - mae: 1.6628\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.0121 - mae: 1.6661\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.8960 - mae: 1.6547\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.2587 - mae: 1.6220\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.6695 - mae: 1.6312\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.6092 - mae: 1.6249\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.6127 - mae: 1.6251\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.5188 - mae: 1.6368\n",
      "82/82 [==============================] - 1s 2ms/step\n",
      "Kappa Score: 0.9590530925915066\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 10ms/step - loss: 65.0193 - mae: 4.4296\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 40.5078 - mae: 3.6010\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 34.1228 - mae: 3.5341\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 31.2386 - mae: 3.4271\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 29.4602 - mae: 3.2943\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 27.7321 - mae: 3.0858\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 25.1166 - mae: 2.8957\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 22.2073 - mae: 2.7109\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 20.3336 - mae: 2.5734\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 18.0297 - mae: 2.4301\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 17.1433 - mae: 2.3383\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 15.2413 - mae: 2.2356\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 14.8366 - mae: 2.1958\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 14.0449 - mae: 2.1398\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 13.5506 - mae: 2.0848\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 13.4710 - mae: 2.0602\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.2128 - mae: 1.9850\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.1966 - mae: 1.9781\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.3917 - mae: 1.9680\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.5319 - mae: 1.9144\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 11.4969 - mae: 1.8984\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.7837 - mae: 1.8537\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.0391 - mae: 1.8623\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.8012 - mae: 1.8435\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.3503 - mae: 1.7992\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.7844 - mae: 1.8275\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.2267 - mae: 1.7801\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.2869 - mae: 1.7895\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.8944 - mae: 1.7813\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 10.0024 - mae: 1.7679\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.9776 - mae: 1.7722\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 9.8752 - mae: 1.7494\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.3825 - mae: 1.7216\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.1888 - mae: 1.7034\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.5119 - mae: 1.7421\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.5254 - mae: 1.7302\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.1828 - mae: 1.7023\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9545 - mae: 1.6907\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.0209 - mae: 1.6882\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.8806 - mae: 1.6938\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.9529 - mae: 1.6924\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7173 - mae: 1.6654\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.5994 - mae: 1.6698\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7014 - mae: 1.6677\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.8992 - mae: 1.6747\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.9336 - mae: 1.6694\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.2734 - mae: 1.6276\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.8037 - mae: 1.6577\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.2428 - mae: 1.6208\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.8865 - mae: 1.6492\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9563160723832389\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 10ms/step - loss: 63.5221 - mae: 4.3344\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 40.4004 - mae: 3.5826\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 34.3073 - mae: 3.4755\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 31.3048 - mae: 3.3752\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 29.3288 - mae: 3.2196\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 27.4821 - mae: 3.0489\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 25.7148 - mae: 2.9155\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 21.4407 - mae: 2.6422\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 18.6352 - mae: 2.4490\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 17.0030 - mae: 2.3245\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 16.0708 - mae: 2.2795\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 15.4515 - mae: 2.2175\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 14.4980 - mae: 2.1358\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 14.0627 - mae: 2.1077\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 13.5212 - mae: 2.0450\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 12.7297 - mae: 1.9947\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 12.9923 - mae: 2.0040\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.4855 - mae: 1.9625\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 12.0744 - mae: 1.9381\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 4s 27ms/step - loss: 11.2239 - mae: 1.8814\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 11.5930 - mae: 1.8799\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.4538 - mae: 1.8003\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.5059 - mae: 1.8493\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.9605 - mae: 1.8100\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.8438 - mae: 1.8070\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.4945 - mae: 1.7814\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.0513 - mae: 1.7481\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.4362 - mae: 1.7948\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.1127 - mae: 1.7711\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.3278 - mae: 1.7660\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 10.2711 - mae: 1.7576\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 9.7630 - mae: 1.7294\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.7376 - mae: 1.7267\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.8828 - mae: 1.7294\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.6263 - mae: 1.7126\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.5206 - mae: 1.6939\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.3479 - mae: 1.7029\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.3958 - mae: 1.6988\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.5272 - mae: 1.6950\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.1177 - mae: 1.6729\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.0452 - mae: 1.6770\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.3156 - mae: 1.6896\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.1873 - mae: 1.6755\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.9568 - mae: 1.6704\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2037 - mae: 1.6803\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.9363 - mae: 1.6751\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.8461 - mae: 1.6522\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.6528 - mae: 1.6586\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.8217 - mae: 1.6537\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.0358 - mae: 1.6646\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9569183740200052\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 9ms/step - loss: 65.4394 - mae: 4.3840\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 40.9780 - mae: 3.5700\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 34.4926 - mae: 3.4652\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 31.5672 - mae: 3.3934\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 28.9057 - mae: 3.2223\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 4s 27ms/step - loss: 26.7280 - mae: 3.0449\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 24.3418 - mae: 2.8556\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 22.0072 - mae: 2.6868\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 18.5767 - mae: 2.4713\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 17.4527 - mae: 2.3724\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 16.1592 - mae: 2.2887\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 15.0217 - mae: 2.1883\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 13.9902 - mae: 2.1396\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 13.8385 - mae: 2.0847\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 12.9643 - mae: 2.0357\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 13.3846 - mae: 2.0359\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 12.7708 - mae: 1.9947\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 11.9118 - mae: 1.9501\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 12.0750 - mae: 1.9292\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 11.7493 - mae: 1.9141\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 11.7236 - mae: 1.8828\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 11.1925 - mae: 1.8463\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 10.9694 - mae: 1.8396\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.9302 - mae: 1.8104\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 10.5565 - mae: 1.7959\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.4585 - mae: 1.7903\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 10.3616 - mae: 1.7801\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 10.1366 - mae: 1.7552\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.7727 - mae: 1.7358\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.4584 - mae: 1.7590\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.8656 - mae: 1.7610\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.7691 - mae: 1.7308\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 9.7745 - mae: 1.7296\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.3939 - mae: 1.7101\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 9.3899 - mae: 1.7108\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 9.6702 - mae: 1.7289\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 9.5732 - mae: 1.7161\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 9.3732 - mae: 1.6966\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 9.3255 - mae: 1.6967\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.0442 - mae: 1.6775\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.8706 - mae: 1.6735\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.3974 - mae: 1.6920\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 8.7678 - mae: 1.6608\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.3976 - mae: 1.6895\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.6765 - mae: 1.6595\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.9066 - mae: 1.6567\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 8.5070 - mae: 1.6382\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.9229 - mae: 1.6622\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.7029 - mae: 1.6493\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.7875 - mae: 1.6510\n",
      "82/82 [==============================] - 1s 2ms/step\n",
      "Kappa Score: 0.9623029997584084\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 5s 10ms/step - loss: 65.1286 - mae: 4.3810\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 40.6742 - mae: 3.6131\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 33.6676 - mae: 3.4944\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 31.3999 - mae: 3.4332\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 29.5788 - mae: 3.2856\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 28.0465 - mae: 3.1300\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 25.3951 - mae: 2.9307\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 22.3535 - mae: 2.7235\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 19.5009 - mae: 2.5264\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.4243 - mae: 2.3772\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.7237 - mae: 2.2599\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 15.1580 - mae: 2.2294\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.4262 - mae: 2.1678\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.8200 - mae: 2.0892\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 12.6219 - mae: 2.0301\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.7018 - mae: 1.9980\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 12.6734 - mae: 1.9779\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.9782 - mae: 1.9504\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.4384 - mae: 1.9596\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.8543 - mae: 1.9150\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 11.2959 - mae: 1.8611\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.8812 - mae: 1.8510\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.5336 - mae: 1.8206\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 11.2848 - mae: 1.8593\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.3324 - mae: 1.8010\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.2662 - mae: 1.7873\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.9090 - mae: 1.7639\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.1657 - mae: 1.7663\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7439 - mae: 1.7347\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7863 - mae: 1.7549\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.6065 - mae: 1.7317\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.0866 - mae: 1.7617\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.5645 - mae: 1.7297\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.1936 - mae: 1.7136\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 9.6665 - mae: 1.7294\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.1937 - mae: 1.7091\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.6058 - mae: 1.7261\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.3245 - mae: 1.7050\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7640 - mae: 1.6853\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.1197 - mae: 1.6862\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.5834 - mae: 1.7227\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.5415 - mae: 1.6708\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.9915 - mae: 1.6870\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 8.6330 - mae: 1.6685\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.2468 - mae: 1.6424\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.5568 - mae: 1.6485\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7245 - mae: 1.6487\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.8870 - mae: 1.6697\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.6389 - mae: 1.6526\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.4904 - mae: 1.6454\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9555032985392117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawansapkota/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from collections.abc import Mapping\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "            \n",
    "    # Initializing variables for Word2Vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represents one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    # lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.958\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear@CAPS1 @CAPS2, I believe that using comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay\n",
       "0  Dear@CAPS1 @CAPS2, I believe that using comput..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = [\"Dear@CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many people have myspace, facebooks, aim, these all benefit us by having conversations with one another. Many people believe computers are bad but how can you make friends if you can never talk to them? I am very fortunate for having a computer that can help with not only school work but my social life and how I make friends. Computers help us with finding our locations, coordibates and millions of information online. If we didn't go on the internet a lot we wouldn't know how to go onto websites that @MONTH1 help us with locations and coordinates like @LOCATION1. Would you rather use a computer or be in @LOCATION3. When your supposed to be vacationing in @LOCATION2. Million of information is found on the internet. You can as almost every question and a computer will have it. Would you rather easily draw up a house plan on the computers or take @NUM1 hours doing one by hand with ugly erazer marks all over it, you are garrenteed that to find a job with a drawing like that. Also when appling for a job many workers must write very long papers like a @NUM3 word essay on why this job fits you the most, and many people I know don't like writing @NUM3 words non-stopp for hours when it could take them I hav an a computer. That is why computers we needed a lot now adays. I hope this essay has impacted your descion on computers because they are great machines to work with. The other day I showed my mom how to use a computer and she said it was the greatest invention sense sliced bread! Now go out and buy a computer to help you chat online with friends, find locations and millions of information on one click of the button and help your self with getting a job with neat, prepared, printed work that your boss will love.\"]\n",
    "demo_df = pd.DataFrame(demo,columns=['essay'])\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(demo_df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"I believe the computer is a machine that is indispensable in our lives. It enables us to benefit in numerous ways, such as communication through platforms like Facebook and MySpace. Utilizing computers allows us to access vast amounts of information, find coordinates and locations, and streamline tasks such as planning house designs and writing reports for work. Let's delve into the wonderful world of technology.Using a computer enriches our lives by facilitating online communication and friendships. Many popular platforms like MySpace, Facebook, and AIM enable us to engage in conversations with people worldwide. Despite some negative perceptions, computers are essential for fostering social connections. How else could we make friends with people we can't physically meet?I am grateful for the role computers play not only in academic pursuits but also in enhancing my social interactions and friendships. Computers serve as indispensable tools for accessing information, locating places, and finding coordinates online. Without regular internet access, we would miss out on valuable resources such as GPS services like @LOCATION1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vecmodel.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m      3\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vecmodel.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m clean_test_essays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m clean_test_essays\u001b[38;5;241m.\u001b[39mappend(essay_to_wordlist( content, remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m ))\n",
      "File \u001b[0;32m~/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/gensim/models/keyedvectors.py:1434\u001b[0m, in \u001b[0;36mWord2VecKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the input-hidden weight matrix from the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124;03mWarnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \n\u001b[1;32m   1432\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# from gensim.models.word2vec import load_word2vec_format\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/gensim/models/utils_any2vec.py:171\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    168\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m    170\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m    172\u001b[0m     header \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(fin\u001b[38;5;241m.\u001b[39mreadline(), encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m    173\u001b[0m     vocab_size, vector_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header\u001b[38;5;241m.\u001b[39msplit())  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:503\u001b[0m, in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    501\u001b[0m     compression \u001b[38;5;241m=\u001b[39m so_compression\u001b[38;5;241m.\u001b[39mINFER_FROM_EXTENSION\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs, url, message, ignore_extension\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vecmodel.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "num_features = 300\n",
    "      \n",
    "model = KeyedVectors.load_word2vec_format( \"word2vecmodel.bin\", binary=True)\n",
    "clean_test_essays = []\n",
    "clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "# lstm_model = get_model()\n",
    "lstm_model.load_weights(\"model_weights/final_lstm.h5\")\n",
    "preds = lstm_model.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_1562/3512985329.py:1: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  int(np.around(preds))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.around(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2595,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = y_pred.T.reshape(2595,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  7.,  9., ..., 28., 32., 31.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
