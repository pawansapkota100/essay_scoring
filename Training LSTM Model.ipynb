{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.9/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# from gensim.models import Word2Vec\n",
    "from collections.abc import Mapping\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dropout,Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "\n",
    "# def get_model():\n",
    "#     \"\"\"Define the model.\"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=(1, 300), return_sequences=True))\n",
    "#     model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "#     model.summary()\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_43208/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 3s 7ms/step - loss: 64.4674 - mae: 4.3888\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 40.5782 - mae: 3.5724\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 34.0495 - mae: 3.4526\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 30.3401 - mae: 3.3559\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 28.5077 - mae: 3.1971\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 27.2215 - mae: 3.0877\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 25.1808 - mae: 2.8852\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 22.0596 - mae: 2.6850\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 19.1689 - mae: 2.5301\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 17.6370 - mae: 2.4010\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 15.9389 - mae: 2.2796\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.8938 - mae: 2.2312\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.9453 - mae: 2.1461\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.5013 - mae: 2.1506\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 13.5073 - mae: 2.0668\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.8773 - mae: 2.0366\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.4815 - mae: 1.9958\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.4998 - mae: 1.9732\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.6931 - mae: 1.9510\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 12.1041 - mae: 1.9322\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.5298 - mae: 1.8879\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.9054 - mae: 1.8457\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.7655 - mae: 1.8415\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.5977 - mae: 1.8109\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.5982 - mae: 1.8131\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.1135 - mae: 1.7592\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.8401 - mae: 1.7681\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.9776 - mae: 1.7551\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.3586 - mae: 1.7847\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.8088 - mae: 1.7477\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.1497 - mae: 1.7785\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.8889 - mae: 1.7414\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.2907 - mae: 1.7114\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.3094 - mae: 1.7119\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.2389 - mae: 1.7198\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.4196 - mae: 1.7123\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.3949 - mae: 1.7177\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.1528 - mae: 1.6766\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.5460 - mae: 1.7020\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9754 - mae: 1.6808\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.0644 - mae: 1.6934\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.0413 - mae: 1.6752\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.9254 - mae: 1.6628\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.8208 - mae: 1.6695\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.5866 - mae: 1.6421\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.9862 - mae: 1.6444\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.2928 - mae: 1.6230\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.3777 - mae: 1.6207\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.1519 - mae: 1.6135\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.6682 - mae: 1.6305\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9596530299988087\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_43208/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 3s 7ms/step - loss: 61.3252 - mae: 4.3013\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 38.6270 - mae: 3.5609\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 32.8462 - mae: 3.4405\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 30.1267 - mae: 3.3790\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 28.9009 - mae: 3.2411\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 27.0860 - mae: 3.0734\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 25.0969 - mae: 2.9241\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 22.1963 - mae: 2.7174\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 19.1410 - mae: 2.5176\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 17.1353 - mae: 2.3688\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 16.2932 - mae: 2.2824\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 14.9968 - mae: 2.2141\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 14.1859 - mae: 2.1484\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 14.1831 - mae: 2.1343\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 13.3536 - mae: 2.0540\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.8199 - mae: 2.0103\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.4544 - mae: 1.9964\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.4869 - mae: 1.9743\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 12.2200 - mae: 1.9271\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.9901 - mae: 1.9208\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.4060 - mae: 1.8786\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 11.1786 - mae: 1.8576\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.8670 - mae: 1.8384\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.8455 - mae: 1.8169\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.9767 - mae: 1.8333\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.4853 - mae: 1.7862\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.5294 - mae: 1.7874\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.2905 - mae: 1.7750\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.9507 - mae: 1.7731\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 10.2083 - mae: 1.7624\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.6380 - mae: 1.7273\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.7016 - mae: 1.7423\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.6513 - mae: 1.7424\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.4795 - mae: 1.7340\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.0748 - mae: 1.7109\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.6389 - mae: 1.7348\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2087 - mae: 1.6944\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.4035 - mae: 1.7077\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2176 - mae: 1.7170\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 8.6646 - mae: 1.6673\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.2002 - mae: 1.6827\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 8.8361 - mae: 1.6637\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.7194 - mae: 1.6702\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.9277 - mae: 1.6794\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.0621 - mae: 1.6857\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7105 - mae: 1.6689\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 8.7310 - mae: 1.6462\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.1416 - mae: 1.6689\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.6152 - mae: 1.6481\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.4147 - mae: 1.6369\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9545633923783231\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_43208/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 9ms/step - loss: 63.2990 - mae: 4.2789\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 39.6964 - mae: 3.5571\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 33.1951 - mae: 3.4444\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 30.3988 - mae: 3.3341\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 28.4574 - mae: 3.1803\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 27.3870 - mae: 3.0603\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 24.5194 - mae: 2.8439\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 22.0097 - mae: 2.6574\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 18.7697 - mae: 2.4636\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 17.4095 - mae: 2.3469\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 15.3485 - mae: 2.2187\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 15.2515 - mae: 2.1869\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 14.5178 - mae: 2.1321\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 13.5781 - mae: 2.0687\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 5s 33ms/step - loss: 13.0829 - mae: 2.0375\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 12.8531 - mae: 2.0000\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 13.0986 - mae: 2.0088\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 12.3196 - mae: 1.9576\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 12.1664 - mae: 1.9406\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 11.6128 - mae: 1.8863\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 11.1922 - mae: 1.8602\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.8272 - mae: 1.8183\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 10.9869 - mae: 1.8190\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 11.0427 - mae: 1.8116\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.7856 - mae: 1.7777\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 10.5966 - mae: 1.7773\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 10.0861 - mae: 1.7490\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 10.0792 - mae: 1.7465\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.0984 - mae: 1.7477\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.1889 - mae: 1.7533\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.9559 - mae: 1.7437\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.6293 - mae: 1.7189\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.8199 - mae: 1.7212\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.5404 - mae: 1.7093\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.2428 - mae: 1.7060\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.2850 - mae: 1.7090\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.1325 - mae: 1.6840\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.6667 - mae: 1.6515\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.0084 - mae: 1.6706\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.1480 - mae: 1.6460\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.8395 - mae: 1.6496\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.1477 - mae: 1.6645\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.0709 - mae: 1.6621\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.7529 - mae: 1.6246\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.5296 - mae: 1.6308\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.4259 - mae: 1.6097\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 8.6500 - mae: 1.6324\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.6003 - mae: 1.6356\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.2199 - mae: 1.5871\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.0173 - mae: 1.6280\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9592152879576336\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_43208/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_14 (LSTM)              (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 10ms/step - loss: 67.6251 - mae: 4.4726\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 43.0884 - mae: 3.7077\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 35.9698 - mae: 3.5452\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 32.6788 - mae: 3.4822\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 30.7495 - mae: 3.3457\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 28.7684 - mae: 3.1561\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 26.9411 - mae: 3.0159\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 23.8440 - mae: 2.7965\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 20.1668 - mae: 2.5790\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 18.1860 - mae: 2.4234\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 16.9882 - mae: 2.3302\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 15.5222 - mae: 2.2266\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 14.3777 - mae: 2.1843\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 13.7114 - mae: 2.1179\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 13.4333 - mae: 2.0713\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 13.8026 - mae: 2.0971\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 12.9039 - mae: 2.0245\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 13.0233 - mae: 1.9995\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 12.0066 - mae: 1.9506\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.3673 - mae: 1.8973\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 11.4584 - mae: 1.8874\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.9689 - mae: 1.8628\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.0646 - mae: 1.8538\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 11.2034 - mae: 1.8439\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.8589 - mae: 1.8383\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.9143 - mae: 1.8369\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.5345 - mae: 1.8185\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 10.6419 - mae: 1.8072\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 9.9292 - mae: 1.7825\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.9294 - mae: 1.7734\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 10.1859 - mae: 1.7764\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.0152 - mae: 1.7627\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 10.0939 - mae: 1.7718\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.7894 - mae: 1.7644\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.3568 - mae: 1.7356\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 9.8284 - mae: 1.7552\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 9.3071 - mae: 1.7223\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.8719 - mae: 1.7542\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2636 - mae: 1.7114\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7449 - mae: 1.6854\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.5759 - mae: 1.6735\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2922 - mae: 1.7198\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 9.0765 - mae: 1.6872\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.7948 - mae: 1.6926\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.8672 - mae: 1.6899\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 6s 39ms/step - loss: 8.7978 - mae: 1.6832\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 8.8809 - mae: 1.6776\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 8.8260 - mae: 1.6702\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 8.5517 - mae: 1.6577\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 8.8030 - mae: 1.6757\n",
      "82/82 [==============================] - 1s 2ms/step\n",
      "Kappa Score: 0.9554766172882835\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vh/mytxqrrs3yq9z8r8cbk1xqwr0000gn/T/ipykernel_43208/3839781270.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  featureVec = np.add(featureVec,model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 1, 300)            721200    \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 64)                93440     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 814705 (3.11 MB)\n",
      "Trainable params: 814705 (3.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 7s 17ms/step - loss: 64.7387 - mae: 4.3729\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 40.1421 - mae: 3.6412\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 34.5024 - mae: 3.5046\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 31.2089 - mae: 3.4287\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 29.1449 - mae: 3.2759\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 27.5293 - mae: 3.0734\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 25.0600 - mae: 2.8875\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 22.3899 - mae: 2.7164\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 19.5003 - mae: 2.5065\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 17.4314 - mae: 2.3648\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 15.8673 - mae: 2.2571\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 15.3402 - mae: 2.2323\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 14.9887 - mae: 2.1984\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 14.2737 - mae: 2.1521\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 14.6061 - mae: 2.1318\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 13.0241 - mae: 2.0354\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 13.5517 - mae: 2.0223\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.4680 - mae: 1.9825\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.6643 - mae: 1.9773\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.1458 - mae: 1.9306\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 11.6327 - mae: 1.9076\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 12.1758 - mae: 1.9224\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 11.3633 - mae: 1.8559\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 11.1309 - mae: 1.8362\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 11.7682 - mae: 1.8542\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 10.6965 - mae: 1.8175\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.3462 - mae: 1.7819\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.5385 - mae: 1.8073\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 10.5440 - mae: 1.8001\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 4s 22ms/step - loss: 10.2130 - mae: 1.7817\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.9554 - mae: 1.7630\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 10.1581 - mae: 1.7879\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 9.5310 - mae: 1.7287\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 9.7314 - mae: 1.7453\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.4772 - mae: 1.7137\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.8766 - mae: 1.7270\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.7923 - mae: 1.6766\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2518 - mae: 1.7018\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.1932 - mae: 1.7061\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.3878 - mae: 1.7056\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 9.3548 - mae: 1.7095\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.9065 - mae: 1.6725\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.7895 - mae: 1.6502\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.3176 - mae: 1.6995\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 9.2185 - mae: 1.6949\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.9241 - mae: 1.6769\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.5802 - mae: 1.6580\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 8.9405 - mae: 1.6650\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 9.2480 - mae: 1.6839\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 8.9712 - mae: 1.6561\n",
      "82/82 [==============================] - 0s 2ms/step\n",
      "Kappa Score: 0.9624617125841323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawansapkota/Desktop/Automated-Essay--Scoring/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from collections.abc import Mapping\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "            \n",
    "    # Initializing variables for Word2Vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represents one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    # lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9583\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
